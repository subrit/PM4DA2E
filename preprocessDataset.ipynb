{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets\n",
    "#pip install IProgress\n",
    "#conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "#pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xnli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'en' at C:\\Users\\Me\\.cache\\huggingface\\datasets\\xnli\\en\\0.0.0\\b8dd5d7af51114dbda02c0e3f6133f332186418e (last modified on Thu Jan 16 11:14:05 2025).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#Using XNLI dataset - English\n",
    "dataset = load_dataset('xnli', 'en')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premise = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['premise']]\n",
    "    hypothesis = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['hypothesis']]\n",
    "   \n",
    "    return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "#dataset[\"train\"][\"premise\"]\n",
    "\n",
    "vlt = len(tokenized_datasets['train'])\n",
    "#vlt = 1000\n",
    "#vlt\n",
    "\n",
    "vlv = len(tokenized_datasets['validation'])\n",
    "#vlv = 250\n",
    "#vlv\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_indices = random.sample(range(len(tokenized_datasets['train'])), vlt) #392702\n",
    "val_indices = random.sample(range(len(tokenized_datasets['validation'])), vlv) #2490\n",
    "\n",
    "train_dataset = tokenized_datasets['train'].select(train_indices)\n",
    "val_dataset = tokenized_datasets['validation'].select(val_indices)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xnli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'fr' at C:\\Users\\Me\\.cache\\huggingface\\datasets\\xnli\\fr\\0.0.0\\b8dd5d7af51114dbda02c0e3f6133f332186418e (last modified on Thu Jan 16 11:24:19 2025).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#Using XNLI dataset - french\n",
    "dataset = load_dataset('xnli', 'fr')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premise = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['premise']]\n",
    "    hypothesis = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['hypothesis']]\n",
    "   \n",
    "    return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "#dataset[\"train\"][\"premise\"]\n",
    "\n",
    "vlt = len(tokenized_datasets['train'])\n",
    "#vlt = 1000\n",
    "#vlt\n",
    "\n",
    "vlv = len(tokenized_datasets['validation'])\n",
    "#vlv = 250\n",
    "#vlv\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_indices = random.sample(range(len(tokenized_datasets['train'])), vlt) #392702\n",
    "val_indices = random.sample(range(len(tokenized_datasets['validation'])), vlv) #2490\n",
    "\n",
    "train_dataset = tokenized_datasets['train'].select(train_indices)\n",
    "val_dataset = tokenized_datasets['validation'].select(val_indices)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xnli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'es' at C:\\Users\\Me\\.cache\\huggingface\\datasets\\xnli\\es\\0.0.0\\b8dd5d7af51114dbda02c0e3f6133f332186418e (last modified on Thu Jan 16 17:44:52 2025).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#Using XNLI dataset - Spanish\n",
    "dataset = load_dataset('xnli', 'es')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premise = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['premise']]\n",
    "    hypothesis = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['hypothesis']]\n",
    "   \n",
    "    return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "#dataset[\"train\"][\"premise\"]\n",
    "\n",
    "vlt = len(tokenized_datasets['train'])\n",
    "#vlt = 1000\n",
    "#vlt\n",
    "\n",
    "vlv = len(tokenized_datasets['validation'])\n",
    "#vlv = 250\n",
    "#vlv\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_indices = random.sample(range(len(tokenized_datasets['train'])), vlt) #392702\n",
    "val_indices = random.sample(range(len(tokenized_datasets['validation'])), vlv) #2490\n",
    "\n",
    "train_dataset = tokenized_datasets['train'].select(train_indices)\n",
    "val_dataset = tokenized_datasets['validation'].select(val_indices)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xnli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'hi' at C:\\Users\\Me\\.cache\\huggingface\\datasets\\xnli\\hi\\0.0.0\\b8dd5d7af51114dbda02c0e3f6133f332186418e (last modified on Thu Jan 16 12:22:40 2025).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#Using XNLI dataset - hindi\n",
    "dataset = load_dataset('xnli', 'hi')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premise = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['premise']]\n",
    "    hypothesis = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['hypothesis']]\n",
    "   \n",
    "    return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "#dataset[\"train\"][\"premise\"]\n",
    "\n",
    "vlt = len(tokenized_datasets['train'])\n",
    "#vlt = 1000\n",
    "#vlt\n",
    "\n",
    "vlv = len(tokenized_datasets['validation'])\n",
    "#vlv = 250\n",
    "#vlv\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_indices = random.sample(range(len(tokenized_datasets['train'])), vlt) #392702\n",
    "val_indices = random.sample(range(len(tokenized_datasets['validation'])), vlv) #2490\n",
    "\n",
    "train_dataset = tokenized_datasets['train'].select(train_indices)\n",
    "val_dataset = tokenized_datasets['validation'].select(val_indices)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xnli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'de' at C:\\Users\\Me\\.cache\\huggingface\\datasets\\xnli\\de\\0.0.0\\b8dd5d7af51114dbda02c0e3f6133f332186418e (last modified on Thu Jan 16 17:37:12 2025).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#Using XNLI dataset - german\n",
    "dataset = load_dataset('xnli', 'de')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premise = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['premise']]\n",
    "    hypothesis = [ex if isinstance(ex, str) else \" \".join(ex) for ex in examples['hypothesis']]\n",
    "   \n",
    "    return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "#dataset[\"train\"][\"premise\"]\n",
    "\n",
    "vlt = len(tokenized_datasets['train'])\n",
    "#vlt = 1000\n",
    "#vlt\n",
    "\n",
    "vlv = len(tokenized_datasets['validation'])\n",
    "#vlv = 250\n",
    "#vlv\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_indices = random.sample(range(len(tokenized_datasets['train'])), vlt) #392702\n",
    "val_indices = random.sample(range(len(tokenized_datasets['validation'])), vlv) #2490\n",
    "\n",
    "train_dataset = tokenized_datasets['train'].select(train_indices)\n",
    "val_dataset = tokenized_datasets['validation'].select(val_indices)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
